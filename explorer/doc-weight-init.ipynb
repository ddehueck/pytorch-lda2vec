{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ddehueck/.local/share/Trash/files/explorer.3\r\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def preprocess(docs, nlp, min_length, min_counts, max_counts, tokenize_fn):\n",
    "    \"\"\"Tokenize, clean, and encode documents.\n",
    "    Arguments:\n",
    "        docs: A list of tuples (index, string), each string is a document.\n",
    "        nlp: A spaCy object, like nlp = spacy.load('en').\n",
    "        min_length: An integer, minimum document length.\n",
    "        min_counts: An integer, minimum count of a word.\n",
    "        max_counts: An integer, maximum count of a word.\n",
    "    Returns:\n",
    "        encoded_docs: A list of tuples (index, list), each list is a document\n",
    "            with words encoded by integer values.\n",
    "        decoder: A dict, integer -> word.\n",
    "        word_counts: A list of integers, counts of words that are in decoder.\n",
    "            word_counts[i] is the number of occurrences of word decoder[i]\n",
    "            in all documents in docs.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_docs = [(i, tokenize_fn(doc)) for i, doc in tqdm(docs)]\n",
    "\n",
    "    # remove short documents\n",
    "    n_short_docs = sum(1 for i, doc in tokenized_docs if len(doc) < min_length)\n",
    "    tokenized_docs = [(i, doc) for i, doc in tokenized_docs if len(doc) >= min_length]\n",
    "    print('number of removed short documents:', n_short_docs)\n",
    "\n",
    "    # remove some tokens\n",
    "    counts = _count_unique_tokens(tokenized_docs)\n",
    "    tokenized_docs = _remove_tokens(tokenized_docs, counts, min_counts, max_counts)\n",
    "    n_short_docs = sum(1 for i, doc in tokenized_docs if len(doc) < min_length)\n",
    "    tokenized_docs = [(i, doc) for i, doc in tokenized_docs if len(doc) >= min_length]\n",
    "    print('number of additionally removed short documents:', n_short_docs)\n",
    "\n",
    "    counts = _count_unique_tokens(tokenized_docs)\n",
    "    encoder, decoder, word_counts = _create_token_encoder(counts)\n",
    "\n",
    "    print('\\nminimum word count number:', word_counts[-1])\n",
    "    print('this number can be less than MIN_COUNTS because of document removal')\n",
    "\n",
    "    encoded_docs = _encode(tokenized_docs, encoder)\n",
    "    return encoded_docs, decoder, word_counts\n",
    "\n",
    "\n",
    "def _count_unique_tokens(tokenized_docs):\n",
    "    tokens = []\n",
    "    for i, doc in tokenized_docs:\n",
    "        tokens += doc\n",
    "    return Counter(tokens)\n",
    "\n",
    "\n",
    "def _encode(tokenized_docs, encoder):\n",
    "    return [(i, [encoder[t] for t in doc]) for i, doc in tokenized_docs]\n",
    "\n",
    "\n",
    "def _remove_tokens(tokenized_docs, counts, min_counts, max_counts):\n",
    "    \"\"\"\n",
    "    Words with count < min_counts or count > max_counts\n",
    "    will be removed.\n",
    "    \"\"\"\n",
    "    total_tokens_count = sum(\n",
    "        count for token, count in counts.most_common()\n",
    "    )\n",
    "    print('total number of tokens:', total_tokens_count)\n",
    "\n",
    "    unknown_tokens_count = sum(\n",
    "        count for token, count in counts.most_common()\n",
    "        if count < min_counts or count > max_counts\n",
    "    )\n",
    "    print('number of tokens to be removed:', unknown_tokens_count)\n",
    "\n",
    "    keep = {}\n",
    "    for token, count in counts.most_common():\n",
    "        keep[token] = count >= min_counts and count <= max_counts\n",
    "\n",
    "    return [(i, [t for t in doc if keep[t]]) for i, doc in tokenized_docs]\n",
    "\n",
    "\n",
    "def _create_token_encoder(counts):\n",
    "\n",
    "    total_tokens_count = sum(\n",
    "        count for token, count in counts.most_common()\n",
    "    )\n",
    "    print('total number of tokens:', total_tokens_count)\n",
    "\n",
    "    encoder = {}\n",
    "    decoder = {}\n",
    "    word_counts = []\n",
    "    i = 0\n",
    "\n",
    "    for token, count in counts.most_common():\n",
    "        # counts.most_common() is in decreasing count order\n",
    "        encoder[token] = i\n",
    "        decoder[i] = token\n",
    "        word_counts.append(count)\n",
    "        i += 1\n",
    "\n",
    "    return encoder, decoder, word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def og_tokenize(doc):\n",
    "    text = ' '.join(doc.split())  # remove excessive spaces\n",
    "    text = nlp(text, disable=['parse', 'entity'])\n",
    "    return [t.lemma_.lower() for t in text if t.is_alpha and len(t) > 2 and not t.is_stop]\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from datasets.preprocess import Tokenizer\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.nlp = None\n",
    "        \n",
    "tokenizer = Tokenizer(merge_noun_chunks=True)\n",
    "\n",
    "def my_tokenize(doc):\n",
    "    return tokenizer.tokenize_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "docs = [(i, doc) for i, doc in enumerate(dataset['data'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944c5e0e6b05491a89831d13081ea68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18846), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of removed short documents: 3979\n",
      "total number of tokens: 1441495\n",
      "number of tokens to be removed: 395483\n",
      "number of additionally removed short documents: 2052\n",
      "total number of tokens: 1022282\n",
      "\n",
      "minimum word count number: 13\n",
      "this number can be less than MIN_COUNTS because of document removal\n"
     ]
    }
   ],
   "source": [
    "encoded_docs, decoder, word_counts = preprocess(docs, nlp, 15, 20, 1800, og_tokenize)\n",
    "\n",
    "# OG TOKENIZE\n",
    "og_texts = [[decoder[j] for j in doc] for i, doc in encoded_docs]\n",
    "og_dictionary = corpora.Dictionary(og_texts)\n",
    "og_corpus = [og_dictionary.doc2bow(text) for text in og_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7435\n"
     ]
    }
   ],
   "source": [
    "print(len(decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7673dd4720d74aa1b461d5fd86e70767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18846), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of removed short documents: 4885\n",
      "total number of tokens: 1225018\n",
      "number of tokens to be removed: 490453\n",
      "number of additionally removed short documents: 3060\n",
      "total number of tokens: 700663\n",
      "\n",
      "minimum word count number: 12\n",
      "this number can be less than MIN_COUNTS because of document removal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded_docs, decoder, word_counts = preprocess(docs, nlp, 15, 20, 1800, my_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MY TOKENIZE\n",
    "my_texts = [[decoder[j] for j in doc] for i, doc in encoded_docs]\n",
    "my_dictionary = corpora.Dictionary(my_texts)\n",
    "my_corpus = [my_dictionary.doc2bow(text) for text in my_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7187\n"
     ]
    }
   ],
   "source": [
    "print(len(decoder)) # 9329 without merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "og_lda = models.LdaModel(og_corpus, alpha=0.9, id2word=og_dictionary, num_topics=24)\n",
    "og_corpus_lda = og_lda[og_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "my_lda = models.LdaModel(my_corpus, alpha=0.9, id2word=my_dictionary, num_topics=24)\n",
    "my_corpus_lda = my_lda[my_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My Tokenizer\n",
      "topic 0 : com posting network keyboard service internet newsgroup modem thanks users\n",
      "topic 1 : jesus bible christ word love life church sin faith man\n",
      "topic 2 : gun guns crime control firearms weapon weapons firearm fire fbi\n",
      "topic 3 : thanks science lot actually looking argument post probably logic claim\n",
      "topic 4 : image software color files images graphics version dos mac format\n",
      "topic 5 : list send email ftp pub software faq address computer anonymous\n",
      "topic 6 : period play detroit vancouver san division toronto pts chicago power\n",
      "topic 7 : neutral wire outlets ground wiring tape dog run dos usually\n",
      "topic 8 : israel israeli jews state arab rights jewish peace land case\n",
      "topic 9 : armenian armenians turkish greek turkey war turks children russian genocide\n",
      "topic 10 : window server application set widget motif running run user sun\n",
      "topic 11 : card mhz cpu board video cards price apple monitor ram\n",
      "topic 12 : game team goal ice play blues mark flames season shot\n",
      "topic 13 : game team games hockey players season league myers nhl teams\n",
      "topic 14 : president national american money states administration congress jobs lost public\n",
      "topic 15 : car cars henrik mormon radar sure problems probably detectors let\n",
      "topic 16 : pit det bos win tor min chi van cal mon\n",
      "topic 17 : come religion church world came went let day told says\n",
      "topic 18 : bike thing real sure probably lot helmet price radio car\n",
      "topic 19 : car condition giz price offer sale shipping dealer little asking\n",
      "topic 20 : key chip encryption keys clipper public security des ripem law\n",
      "topic 21 : space earth nasa planet spacecraft solar launch moon surface orbit\n",
      "topic 22 : drive scsi disk drives hard power controller cable pin speed\n",
      "topic 23 : health medical drug drugs insurance disease care patients cancer treatment\n"
     ]
    }
   ],
   "source": [
    "#print(\"OG Method\")\n",
    "#for i, topics in og_lda.show_topics(24, formatted=False):\n",
    "#   print('topic', i, ':', ' '.join([t for t, _ in topics]))\n",
    "    \n",
    "\"\"\"\n",
    "OG Method\n",
    "topic 0 : israel jews country israeli arab jewish war attack public peace\n",
    "topic 1 : play hockey goal watch season guy shot pick fan end\n",
    "topic 2 : bit ripem color chip message des pgp copy rsa encryption\n",
    "topic 3 : book open subject job note library probably openwindows source old\n",
    "topic 4 : bike myer ride food eat motorcycle dog feel road eye\n",
    "topic 5 : window server application client widget font user display motif manager\n",
    "topic 6 : player hit john clutch moncton springfield bad baltimore well providence\n",
    "topic 7 : president job ground house talk white today technology general decision\n",
    "topic 8 : battery signal little lot great bad old turn hear actually\n",
    "topic 9 : jesus church bible christ word sin lord christian faith life\n",
    "topic 10 : insurance pay private car company money care tax buy cost\n",
    "topic 11 : team win play player season league nhl hockey period toronto\n",
    "topic 12 : armenian armenians turkish kill turkey woman greek leave live village\n",
    "topic 13 : car printer buy driver engine phone print sell laser dealer\n",
    "topic 14 : gun drug kinsey homosexual health rate child disease report safety\n",
    "topic 15 : belief argument evidence atheist exist religion claim true reason example\n",
    "topic 16 : university space book planet water world earth moon technology nuclear\n",
    "topic 17 : software version ftp datum user package computer tool graphic code\n",
    "topic 18 : christian moral objective human atheist morality different claim value accept\n",
    "topic 19 : board heat circuit wire cpu picture current quality sink small\n",
    "topic 20 : fire weapon fbi gun child koresh firearm death person compound\n",
    "topic 21 : pitcher pitch bad probably lot average hit morris guy team\n",
    "topic 22 : space launch earth orbit mission satellite nasa center spacecraft solar\n",
    "topic 23 : card disk dos price windows hard controller monitor board speed\n",
    "\"\"\"\n",
    "\n",
    "print()    \n",
    "\n",
    "print(\"My Tokenizer\")\n",
    "for i, topics in my_lda.show_topics(24, formatted=False):\n",
    "    print('topic', i, ':', ' '.join([t for t, _ in topics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607112e8b13a49ffb2ebe189e52ef8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13434), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "doc_weights_init = np.zeros((len(my_corpus_lda), 24))\n",
    "for i in tqdm(range(len(my_corpus_lda))):\n",
    "    topics = my_corpus_lda[i]\n",
    "    for j, prob in topics:\n",
    "        doc_weights_init[i, j] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
