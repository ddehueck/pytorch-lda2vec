{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "check_dir = '115239_17082019_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved data\n",
    "model = t.load(f'{check_dir}/checkpoint_60.pth', map_location='cpu')\n",
    "dataset = t.load(f'saved_datasets/20_news_groups_dataset/metadata.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(model['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportions(doc_weights):\n",
    "    \"\"\"\n",
    "    Softmax document weights to get proportions\n",
    "    \"\"\"\n",
    "    return F.softmax(doc_weights, dim=1).unsqueeze(dim=2)\n",
    "\n",
    "def get_doc_vectors(doc_weights, topic_embeds):\n",
    "    \"\"\"\n",
    "    Multiply by proportions by topic embeddings to get document vectors\n",
    "    \"\"\"\n",
    "    proportions = get_proportions(doc_weights)\n",
    "    doc_vecs = (proportions * topic_embeds.unsqueeze(0)).sum(dim=1)\n",
    "\n",
    "    return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17652, 300])\n"
     ]
    }
   ],
   "source": [
    "topic_embeds = model[\"model_state_dict\"][\"topic_embeds\"]\n",
    "word_embeds = model[\"model_state_dict\"][\"word_embeds.weight\"]\n",
    "doc_weights = model[\"model_state_dict\"][\"doc_weights.weight\"]\n",
    "\n",
    "vocab = list(dataset['term_freq_dict'].keys())\n",
    "term_freq = list(dataset['term_freq_dict'].values())\n",
    "doc_lens = dataset['doc_lengths']\n",
    "\n",
    "doc_embeds = get_doc_vectors(doc_weights, topic_embeds)\n",
    "\n",
    "print(doc_embeds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordvec2idx(word_vec):\n",
    "    return np.where(word_embeds.numpy() == word_vec.numpy())[0][0]\n",
    "\n",
    "def vec2word(word_vec):\n",
    "    idx = wordvec2idx(word_vec)\n",
    "    return vocab[idx]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_closest_word_vecs(topic_vec, n=10):\n",
    "    dist = F.cosine_similarity(word_embeds, topic_vec.unsqueeze(dim=1).transpose(0, 1))\n",
    "    index_sorted = dist.argsort()\n",
    "    return index_sorted[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0: fax vector density philadelphia creator fault laughter sickness age destiny\n",
      "TOPIC 1: handle cover restrict music immoral distorted peace fault illegally grace\n",
      "TOPIC 2: allergy age stem density attraction cts maternity erase eisa dec\n",
      "TOPIC 3: fax iifx sleeve healthy sweep bold onset distorted inevitably misplace\n",
      "TOPIC 4: cover iifx accidentally restrict willie tell effect grace have moderately\n",
      "TOPIC 5: misplace iifx moslems pena lourdes government reverse nords brake processor\n",
      "TOPIC 6: associates know misplace email moore diagram builder exposure member buy\n",
      "TOPIC 7: moore produce res sure email have reverse market diagram fault\n",
      "TOPIC 8: moore simple destiny reverse location era brake maternity question distance\n",
      "TOPIC 9: iifx embargo misplace base know government era site maternity totally\n",
      "TOPIC 10: smuggle xwindows otc density hunt necessarily hold daemon pls perform\n",
      "TOPIC 11: finnish iifx exposure text salonica maternity nords speedy destiny smokeless\n",
      "TOPIC 12: destiny member maternity philadelphia fault spin nearly huge exposure distance\n",
      "TOPIC 13: beginner earth acidophilus restrict have sure sequence effect allergy mbyte\n",
      "TOPIC 14: nords destiny exposure eisa brake daemon fax iifx sweep maternity\n",
      "TOPIC 15: iifx destiny base brake pls totally originate distance era know\n",
      "TOPIC 16: res cover reckon price restrict interviews prove dentist maternity scanner\n",
      "TOPIC 17: maternity iifx thanx closely brake associates fault base distance diagram\n",
      "TOPIC 18: theme restrict mmu maternity sure sale instruct embargo firearm arm\n",
      "TOPIC 19: iifx allergy direction center spencer suggest associates rest archer wonder\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(topic_embeds):\n",
    "    # Get 10 closest word_embeds\n",
    "    top_10 = get_n_closest_word_vecs(topic)\n",
    "    print(f'TOPIC {i}: {\" \".join([vocab[vec] for vec in top_10])}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    out = e_x / e_x.sum()\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def _softmax_2d(x):\n",
    "    y = x - x.max(axis=1, keepdims=True)\n",
    "    np.exp(y, out=y)\n",
    "    y /= y.sum(axis=1, keepdims=True)\n",
    "    return y\n",
    "\n",
    "\n",
    "def prob_words(context, vocab, temperature=1.0):\n",
    "    \"\"\" This calculates a softmax over the vocabulary as a function\n",
    "    of the dot product of context and word.\n",
    "    \"\"\"\n",
    "    dot = np.dot(vocab, context)\n",
    "    prob = _softmax(dot / temperature)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "\n",
    "def prepare_topics(weights, factors, word_vectors, vocab, temperature=1.0,\n",
    "                   doc_lengths=None, term_frequency=None, normalize=False):\n",
    "    \"\"\" Collects a dictionary of word, document and topic distributions.\n",
    "    https://github.com/cemoody/lda2vec/blob/b7f4642b750c6e792c07d177bd57ad36e65bb35c/lda2vec/topics.py\n",
    "    Arguments\n",
    "    ---------\n",
    "    weights : float array\n",
    "        This must be an array of unnormalized log-odds of document-to-topic\n",
    "        weights. Shape should be [n_documents, n_topics]\n",
    "    factors : float array\n",
    "        Should be an array of topic vectors. These topic vectors live in the\n",
    "        same space as word vectors and will be used to find the most similar\n",
    "        words to each topic. Shape should be [n_topics, n_dim].\n",
    "    word_vectors : float array\n",
    "        This must be a matrix of word vectors. Should be of shape\n",
    "        [n_words, n_dim]\n",
    "    vocab : list of str\n",
    "        These must be the strings for words corresponding to\n",
    "        indices [0, n_words]\n",
    "    temperature : float\n",
    "        Used to calculate the log probability of a word. Higher\n",
    "        temperatures make more rare words more likely.\n",
    "    doc_lengths : int array\n",
    "        An array indicating the number of words in the nth document.\n",
    "        Must be of shape [n_documents]. Required by pyLDAvis.\n",
    "    term_frequency : int array\n",
    "        An array indicating the overall number of times each token appears\n",
    "        in the corpus. Must be of shape [n_words]. Required by pyLDAvis.\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        This dictionary is readily consumed by pyLDAVis for topic\n",
    "        visualization.\n",
    "    \"\"\"\n",
    "    # Map each factor vector to a word\n",
    "    topic_to_word = []\n",
    "    msg = \"Vocabulary size did not match size of word vectors\"\n",
    "    assert len(vocab) == word_vectors.shape[0], msg\n",
    "    if normalize:\n",
    "        word_vectors /= np.linalg.norm(word_vectors, axis=1)[:, None]\n",
    "    # factors = factors / np.linalg.norm(factors, axis=1)[:, None]\n",
    "    for factor_vector in factors:\n",
    "        factor_to_word = prob_words(factor_vector, word_vectors,\n",
    "                                    temperature=temperature)\n",
    "        topic_to_word.append(np.ravel(factor_to_word))\n",
    "    topic_to_word = np.array(topic_to_word)\n",
    "    msg = \"Not all rows in topic_to_word sum to 1\"\n",
    "    assert np.allclose(np.sum(topic_to_word, axis=1), 1), msg\n",
    "    # Collect document-to-topic distributions, e.g. theta\n",
    "    doc_to_topic = _softmax_2d(weights)\n",
    "    msg = \"Not all rows in doc_to_topic sum to 1\"\n",
    "    assert np.allclose(np.sum(doc_to_topic, axis=1), 1), msg\n",
    "    data = {'topic_term_dists': topic_to_word,\n",
    "            'doc_topic_dists': doc_to_topic,\n",
    "            'doc_lengths': doc_lengths,\n",
    "            'vocab': vocab,\n",
    "            'term_frequency': term_frequency}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17652, 20, 1])\n",
      "torch.Size([300, 20])\n",
      "torch.Size([9006, 300])\n",
      "9006\n",
      "17652\n",
      "16712\n"
     ]
    }
   ],
   "source": [
    "print(get_proportions(doc_weights).size())\n",
    "print(t.transpose(topic_embeds, 0, 1).size())\n",
    "print(word_embeds.size())\n",
    "print(len(vocab))\n",
    "print(len(doc_lens))\n",
    "print(np.max(term_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_pyldavis = prepare_topics(\n",
    "    doc_weights.numpy(),\n",
    "    topic_embeds.numpy(),\n",
    "    word_embeds.numpy(),\n",
    "    np.array(vocab),\n",
    "    doc_lengths=np.array(doc_lens),\n",
    "    term_frequency=np.array(term_freq)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "vis_data = pyLDAvis.prepare(**checkpoint_pyldavis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
