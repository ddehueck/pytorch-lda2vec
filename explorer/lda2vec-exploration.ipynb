{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "check_dir = '115239_17082019_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved data\n",
    "model = t.load(f'{check_dir}/checkpoint_60.pth', map_location='cpu')\n",
    "dataset = t.load(f'saved_datasets/20_news_groups_dataset/metadata.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(model['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportions(doc_weights):\n",
    "    \"\"\"\n",
    "    Softmax document weights to get proportions\n",
    "    \"\"\"\n",
    "    return F.softmax(doc_weights, dim=1).unsqueeze(dim=2)\n",
    "\n",
    "def get_doc_vectors(doc_weights, topic_embeds):\n",
    "    \"\"\"\n",
    "    Multiply by proportions by topic embeddings to get document vectors\n",
    "    \"\"\"\n",
    "    proportions = get_proportions(doc_weights)\n",
    "    doc_vecs = (proportions * topic_embeds.unsqueeze(0)).sum(dim=1)\n",
    "\n",
    "    return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17652, 300])\n"
     ]
    }
   ],
   "source": [
    "topic_embeds = model[\"model_state_dict\"][\"topic_embeds\"]\n",
    "word_embeds = model[\"model_state_dict\"][\"word_embeds.weight\"]\n",
    "doc_weights = model[\"model_state_dict\"][\"doc_weights.weight\"]\n",
    "\n",
    "vocab = list(dataset['term_freq_dict'].keys())\n",
    "term_freq = list(dataset['term_freq_dict'].values())\n",
    "doc_lens = dataset['doc_lengths']\n",
    "\n",
    "doc_embeds = get_doc_vectors(doc_weights, topic_embeds)\n",
    "\n",
    "print(doc_embeds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordvec2idx(word_vec):\n",
    "    return np.where(word_embeds.numpy() == word_vec.numpy())[0][0]\n",
    "\n",
    "def vec2word(word_vec):\n",
    "    idx = wordvec2idx(word_vec)\n",
    "    return vocab[idx]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_closest_word_vecs(topic_vec, n=10):\n",
    "    dist = F.cosine_similarity(word_embeds, topic_vec.unsqueeze(dim=1).transpose(0, 1))\n",
    "    index_sorted = dist.argsort()\n",
    "    return index_sorted[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0: duct fatal equivalent TEMPEST obedience performance page Guard Anybody heater\n",
      "TOPIC 1: underestimate Silicon parallel Land salvation assert influence performance panic config\n",
      "TOPIC 2: Panama Anybody cellular equivalent session pertinent formatting ceremonial electric Analyst\n",
      "TOPIC 3: duct ver Willis moral Old obo print assert prospect Baerga\n",
      "TOPIC 4: Silicon ver law parallel gate ram sexual config invoke wheel\n",
      "TOPIC 5: Baerga ver les genesis withhold allow russian Block turkish room\n",
      "TOPIC 6: Bay regional Baerga sure remark Pens person pad program arc\n",
      "TOPIC 7: remark secret inconvenient well sure invoke russian presumably Pens performance\n",
      "TOPIC 8: remark lack heater russian professional Azerbaijan turkish formatting Davidian end\n",
      "TOPIC 9: ver Watt Baerga fun regional allow Azerbaijan confused formatting datum\n",
      "TOPIC 10: muffler Judge launcher equivalent LIFE hill sort een cheat rape\n",
      "TOPIC 11: Jerome ver pad Mountain wit formatting Block tcp heater Johansson\n",
      "TOPIC 12: heater program formatting TEMPEST performance double Engineering Sternlight pad end\n",
      "TOPIC 13: spew DMA deputy parallel invoke well acquire sexual Panama ACCESS\n",
      "TOPIC 14: Block heater pad electric turkish een duct ver Old formatting\n",
      "TOPIC 15: ver heater fun turkish cheat datum excellent end Azerbaijan regional\n",
      "TOPIC 16: inconvenient Silicon bell heavily parallel incomplete recall see formatting veteran\n",
      "TOPIC 17: formatting ver devil term turkish Bay performance fun end Pens\n",
      "TOPIC 18: pro parallel interval formatting well Senate Dave Watt possibly Honda\n",
      "TOPIC 19: ver Panama rider commercial Gilmour Speedstar Bay Ultra legitimately present\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(topic_embeds):\n",
    "    # Get 10 closest word_embeds\n",
    "    top_10 = get_n_closest_word_vecs(topic)\n",
    "    print(f'TOPIC {i}: {\" \".join([vocab[vec] for vec in top_10])}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    out = e_x / e_x.sum()\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def _softmax_2d(x):\n",
    "    y = x - x.max(axis=1, keepdims=True)\n",
    "    np.exp(y, out=y)\n",
    "    y /= y.sum(axis=1, keepdims=True)\n",
    "    return y\n",
    "\n",
    "\n",
    "def prob_words(context, vocab, temperature=1.0):\n",
    "    \"\"\" This calculates a softmax over the vocabulary as a function\n",
    "    of the dot product of context and word.\n",
    "    \"\"\"\n",
    "    dot = np.dot(vocab, context)\n",
    "    prob = _softmax(dot / temperature)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "\n",
    "def prepare_topics(weights, factors, word_vectors, vocab, temperature=1.0,\n",
    "                   doc_lengths=None, term_frequency=None, normalize=False):\n",
    "    \"\"\" Collects a dictionary of word, document and topic distributions.\n",
    "    https://github.com/cemoody/lda2vec/blob/b7f4642b750c6e792c07d177bd57ad36e65bb35c/lda2vec/topics.py\n",
    "    Arguments\n",
    "    ---------\n",
    "    weights : float array\n",
    "        This must be an array of unnormalized log-odds of document-to-topic\n",
    "        weights. Shape should be [n_documents, n_topics]\n",
    "    factors : float array\n",
    "        Should be an array of topic vectors. These topic vectors live in the\n",
    "        same space as word vectors and will be used to find the most similar\n",
    "        words to each topic. Shape should be [n_topics, n_dim].\n",
    "    word_vectors : float array\n",
    "        This must be a matrix of word vectors. Should be of shape\n",
    "        [n_words, n_dim]\n",
    "    vocab : list of str\n",
    "        These must be the strings for words corresponding to\n",
    "        indices [0, n_words]\n",
    "    temperature : float\n",
    "        Used to calculate the log probability of a word. Higher\n",
    "        temperatures make more rare words more likely.\n",
    "    doc_lengths : int array\n",
    "        An array indicating the number of words in the nth document.\n",
    "        Must be of shape [n_documents]. Required by pyLDAvis.\n",
    "    term_frequency : int array\n",
    "        An array indicating the overall number of times each token appears\n",
    "        in the corpus. Must be of shape [n_words]. Required by pyLDAvis.\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        This dictionary is readily consumed by pyLDAVis for topic\n",
    "        visualization.\n",
    "    \"\"\"\n",
    "    # Map each factor vector to a word\n",
    "    topic_to_word = []\n",
    "    msg = \"Vocabulary size did not match size of word vectors\"\n",
    "    assert len(vocab) == word_vectors.shape[0], msg\n",
    "    if normalize:\n",
    "        word_vectors /= np.linalg.norm(word_vectors, axis=1)[:, None]\n",
    "    # factors = factors / np.linalg.norm(factors, axis=1)[:, None]\n",
    "    for factor_vector in factors:\n",
    "        factor_to_word = prob_words(factor_vector, word_vectors,\n",
    "                                    temperature=temperature)\n",
    "        topic_to_word.append(np.ravel(factor_to_word))\n",
    "    topic_to_word = np.array(topic_to_word)\n",
    "    msg = \"Not all rows in topic_to_word sum to 1\"\n",
    "    assert np.allclose(np.sum(topic_to_word, axis=1), 1), msg\n",
    "    # Collect document-to-topic distributions, e.g. theta\n",
    "    doc_to_topic = _softmax_2d(weights)\n",
    "    msg = \"Not all rows in doc_to_topic sum to 1\"\n",
    "    assert np.allclose(np.sum(doc_to_topic, axis=1), 1), msg\n",
    "    data = {'topic_term_dists': topic_to_word,\n",
    "            'doc_topic_dists': doc_to_topic,\n",
    "            'doc_lengths': doc_lengths,\n",
    "            'vocab': vocab,\n",
    "            'term_frequency': term_frequency}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17652, 20, 1])\n",
      "torch.Size([300, 20])\n",
      "torch.Size([9006, 300])\n",
      "9653\n",
      "17577\n",
      "16679\n"
     ]
    }
   ],
   "source": [
    "print(get_proportions(doc_weights).size())\n",
    "print(t.transpose(topic_embeds, 0, 1).size())\n",
    "print(word_embeds.size())\n",
    "print(len(vocab))\n",
    "print(len(doc_lens))\n",
    "print(np.max(term_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Vocabulary size did not match size of word vectors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d39ae10a2647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mterm_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-9-56d50b798d62>\u001b[0m in \u001b[0;36mprepare_topics\u001b[0;34m(weights, factors, word_vectors, vocab, temperature, doc_lengths, term_frequency, normalize)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtopic_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Vocabulary size did not match size of word vectors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mword_vectors\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Vocabulary size did not match size of word vectors"
     ]
    }
   ],
   "source": [
    "checkpoint_pyldavis = prepare_topics(\n",
    "    doc_weights.numpy(),\n",
    "    topic_embeds.numpy(),\n",
    "    word_embeds.numpy(),\n",
    "    np.array(vocab),\n",
    "    doc_lengths=np.array(doc_lens),\n",
    "    term_frequency=np.array(term_freq)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "vis_data = pyLDAvis.prepare(**checkpoint_pyldavis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
